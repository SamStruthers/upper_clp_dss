---
title: "Testing NP preds"
author: "Matthew Ross"
date: "2024-08-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("src/setup_libraries.R")

```


# Data read

```{r}
z <- read_csv('data/parsed_data_ROSS_virridy.csv') %>%
   #select(2:16)%>%
    rename(id = mw_id) %>%
    mutate(id = as.character(id)) %>%
    mutate(fc = FDOM/(Chl_a + Sensor_Turb + FDOM))%>%
  mutate(clean_sensor_datetime = parse_date_time(sensor_datetime, orders = c("%Y-%m-%d %H:%M:%S", "%m/%d/%y %H:%M","%Y-%m-%d" )))%>%
  mutate(sensor_datetime = force_tz(clean_sensor_datetime, tzone = 'MST'))




z_norm <- z %>%
    select( -Kjeldahl, -NO3, -TN, -Phosphorus, -DOC, -clean_sensor_datetime, -Turbidity, -Conductivity, -Depth, -Cl) %>%
    mutate(across(where(is.numeric),scales::rescale)) %>%
    mutate(TOC = z$TOC)%>%
    na.omit()


```


```{r}
#give me the counts by site
# z_norm %>%
#     group_by(id) %>%
#     summarize(count = n()) %>%
#     arrange(desc(count)) %>%
#     mutate(id = factor(id, levels = id)) %>%
#     ggplot(aes(x = id, y = count)) + 
#     geom_col() + 
#     coord_flip() +
#     labs(x = 'Site', y = 'Count') +
#     ROSS_theme
# 
# #counts by month
# z_norm %>%
#     mutate(month = month(sensor_datetime),
#            year = year(sensor_datetime)) %>%
#     group_by(month) %>%
#     summarize(count = n()) %>%
#     arrange(desc(count)) %>%
#     mutate(month = factor(month, levels = month)) %>%
#     ggplot(aes(x = month, y = count)) + 
#     geom_col() + 
#     coord_flip() +
#     labs(x = 'Month', y = 'Count') +
#     ROSS_theme
```


## Train Test Splits

```{r}
#set.seed(100)

  
test_sites <- z_norm %>%
    filter(id %in% c('179', "105075538")) 

test_times <- z_norm%>%
    mutate(month = month(sensor_datetime),
           year = year(sensor_datetime)) %>%
    filter(month %in% c(6,11))

val_test <- bind_rows(test_sites,
                       test_times %>% anti_join(test_sites))

#Checking train vs test split
nrow(val_test)/nrow(z_norm)

# In case we want to do full train/test/val once we have more data, but for now I just kept these as the same (e.g. only train/test). Train/test/val is more robust, but this is fine for the amount of data we have. 

val <- val_test

test <- val_test 

train <- anti_join(z_norm,val_test)
```

# Model

## Model prep
```{r}
features <- c('FDOM','hrs_since_last_cleaning', 'Sensor_Turb', 'Sensor_Cond', 'Chl_a','Temp','fc')
target <- 'TOC'


train_xg <- xgb.DMatrix(data = as.matrix(train[,features]), 
                      label = as.matrix(train[,target]))

val_xg <- xgb.DMatrix(data = as.matrix(val[,features]), 
                     label = as.matrix(val[,target]))

test_xg <-  xgb.DMatrix(data = as.matrix(test[,features]), 
                     label = as.matrix(test[,target]))
```


FROM SAM SILLEN TO HYPER TUNE RS MODEL: https://zenodo.org/records/11582790

```{r}

grid_train <- expand.grid(
  max_depth= c(2,3,4),
  subsample = c(.5,.8,1),
  colsample_bytree= c(.5,.8,1),
  eta = c(.01,0.05, 0.1),
  min_child_weight= c(1,3,5)
)

hypertune_xgboost = function(train,test, grid){
  
  params <- list(booster = "gbtree", objective = "reg:squarederror", eta=grid$eta ,max_depth=grid$max_depth, 
                 min_child_weight=grid$min_child_weight, subsample=grid$subsample, colsample_bytree=grid$colsample_bytree)
  
  xgb.naive <- xgb.train(params = params, data = train, nrounds = 10000, 
                         watchlist = list(train = train, val = test), 
                         print_every_n =1000, early_stopping_rounds = 20)
  
  summary <- grid %>% mutate(val_loss = xgb.naive$best_score, best_message = xgb.naive$best_msg)
  
  return(summary) 
}

## Hypertune xgboost
xgboost_hypertune <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(train_xg,val_xg,current)
  })


best_params <- xgboost_hypertune[xgboost_hypertune$val_loss==min(xgboost_hypertune$val_loss),]
```

```{r}


best_params <- list(booster = "gbtree", objective = "reg:squarederror",
               eta=best_params$eta,
               max_depth=best_params$max_depth, 
               min_child_weight=best_params$min_child_weight, 
               subsample=best_params$subsample, 
               colsample_bytree=best_params$colsample_bytree)
```

```{r}


# params <- list(booster = "gbtree", 
#                objective = "reg:squarederror", 
#                eta=0.05, 
#                gamma=1,
#                subsample = 0.5,
#                max_depth = 3, 
#                min_child_weight=1)


#org run  the boost algo with those settings
toc_mod <- xgb.train(params = best_params, 
                             data = train_xg, 
                             nrounds = 10000,
                             watchlist = list(train = train_xg, 
                                              val = val_xg), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)

importance <- xgb.importance( model = toc_mod)
head(importance)

```

## Apply model

```{r}
test$toc_guess <- predict(toc_mod, test_xg)

ggplot(test, aes(x = TOC, y = toc_guess)) + 
    geom_point()

train$toc_guess <- predict(toc_mod, train_xg)

all <- bind_rows(select(train, TOC, toc_guess, sensor_datetime) %>%
                     mutate(group = 'Train'),
                 select(test, TOC, toc_guess, sensor_datetime) %>%
                     mutate(group = 'Test'))

```


## Visualize model

```{r}

#calculate the MSE
mses <- all %>%
    group_by(group) %>%
    summarize(mse = mean((TOC - toc_guess)^2), 
              rmse = sqrt(mse))%>%
    mutate(mse = round(mse, 3), 
           rmse = round(rmse, 3))

train_rmse <- mses%>%
  filter(group == 'Test')%>%
  pull(rmse)


ROSS_theme <- theme_bw(base_size = 20) + #or theme_few()
    theme(plot.title = element_text(hjust = 0.5, face = 'bold', family = "Roboto"),
          plot.subtitle = element_text(hjust = 0.5, family = "Roboto")) 

all$group <- factor(all$group, levels = c('Train', 'Test'))

ggplot(all, aes(x = TOC, y = toc_guess, color = group, shape = group)) + 
    #add 1:1 line
    geom_abline(intercept = 0, slope = 1, linetype = 'dashed', lwd = 1.5) +
    geom_point(size = 3) + 
  # add a label in the top left corner
  annotate("rect", xmin = 2, xmax = 6, ymin = 8.25, ymax = 10.2, 
             fill = "white", color = "black", alpha = 1, linewidth = 0.5) +
    annotate("text", x = 3.5, y = 9.75, label = paste0("Training samples: n = ", nrow(train)),color = "black", size = 4) +
  annotate("text", x = 3.5, y = 9.25, label = paste0("Testing samples: n = ", nrow(test_times)), color = "black", size = 4) +
    annotate("text", x = 4, y = 8.75, label = paste0("Test Data RMSE: ", train_rmse, " (mg/L)"), color = "black", fontface = 2, size = 4.5) +
  #label for 1:1 line
    scale_color_manual(name = "Model Group", values = c('Train' = "#002EA3", 'Test' = "#E70870"))+
  scale_shape_manual(name = "Model Group", values = c('Train' = 19, 'Test' = 17))+
    ROSS_theme+
  labs( x = "Measured TOC (mg/L)", 
       y = "Model Estimated TOC (mg/L)", 
       color = "Model Group")+
  # put the legend in the bottom right corner of the plot (inside the plot)
  theme(legend.position = c(0.975, 0.05),
          legend.justification = c("right", "bottom"),
          legend.box.background = element_rect(color = "black", fill = "white", linewidth = 0.5),
          legend.margin = margin(6, 6, 6, 6)) +
  xlim(2,10.25)+
  ylim(2,10.25)

#ggsave('images/all_TOC_model_performace.png', width = 7, height = 6, units = 'in', dpi = 500)
```




