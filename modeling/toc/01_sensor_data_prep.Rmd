---
title: "01_data_prep"
author: "Sam Struthers- CSU ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
source("src/setup_libraries.R")

fix_sites <- function(df) {
  fixed_df <- df %>%
    mutate(site = tolower(site)) %>%
    # renaming all the sites, just in case
    mutate(site = case_when(
      grepl("tamasag", site, ignore.case = TRUE) ~ str_replace(site, "tamasag", "bellvue"),
      grepl("legacy", site, ignore.case = TRUE) ~ str_replace(site, "legacy", "salyer"),
      grepl("lincoln", site, ignore.case = TRUE) ~ str_replace(site, "lincoln", "udall"),
       grepl("timberline", site, ignore.case = TRUE) ~ str_replace(site, "timberline", "riverbend"),
      grepl("prospect", site, ignore.case = TRUE) ~ str_replace(site, "prospect", "cottonwood"),
      grepl("timberline", site, ignore.case = TRUE) ~ str_replace(site, "timberline", "riverbend"),
      grepl("prospect", site, ignore.case = TRUE) ~ str_replace(site, "prospect", "cottonwood"),
      grepl("boxelder", site, ignore.case = TRUE) ~ str_replace(site, "boxelder", "elc"),
       grepl("archery", site, ignore.case = TRUE) ~ str_replace(site, "archery", "archery"),
      grepl("archery", site, ignore.case = TRUE) ~ str_replace(site, "archery", "archery"),
      grepl("river bluffs", site, ignore.case = TRUE) ~ str_replace(site, "river bluffs", "riverbluffs"),
      TRUE ~ site)
    )
  return(fixed_df)
}

```


# Clean up sensor data

## Set Data Targets (sites and parameters)

```{r}
site_levels <- c("salyer", "udall", "riverbend", "cottonwood","riverbend_virrridy", "cottonwood_virridy", "elc",
                 "archery", "archery_virridy" ,  "riverbluffs", "joei", "cbri", "chd", "pfal", "sfm", "pbr", "pman", "pbd", "penn", "lbea", "springcreek", "boxcreek")

parameter_levels <- c("Chl-a Fluorescence", "Depth", "DO", "ORP", "pH","Specific Conductivity", "Temperature", "Turbidity", "FDOM Fluorescence") 

all_combinations <- crossing(
  site = site_levels,
  parameter = parameter_levels
)
site_level_string <- paste(site_levels, collapse = "|")
parameter_level_string <- paste(parameter_levels, collapse = "|")
```


merge 2023 + 2024 data for sites OI

# Loading 2023 data

```{r}
#load in 2023 verified data
# Pulling in the manually verified data
verified_names <- list.files(
  here("data", "manual_data_verification", "2023_cycle", 
       "in_progress", "virridy_verification", "verified_directory"),
  full.names = T)

finalized_dataset_2023 <- verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path) %>%
      fix_sites()
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 
# Pulling in the post verified data
post_verified_names <- list.files(
  here("data", "manual_data_verification", "2023_cycle",
       "in_progress", "virridy_verification", "post_verified_directory"),
  full.names = T)
post_finalized_dataset_2023 <- post_verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path) %>%
      fix_sites()
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 
# Combining all 2023 data by preferentially using post verified data
post_names <- intersect(names(post_finalized_dataset_2023), names(finalized_dataset_2023))
non_post_names <- setdiff(names(finalized_dataset_2023), post_names)
dataset_2023 <- c(
  post_finalized_dataset_2023[post_names], 
  finalized_dataset_2023[non_post_names]
)
all_data_2023 <- dataset_2023 %>%
  bind_rows() %>% 
  data.table() %>%
  fix_sites() %>% 
  mutate(
    clean_mean = case_when(
      is.na(flag) & verification_status == "PASS" ~ mean,
      is.na(flag) & verification_status == "FAIL" ~ NA,
      !is.na(flag) & verification_status == "PASS" ~ NA,
      !is.na(flag) & verification_status == "FAIL" ~ mean
    ),
    clean_flag = case_when(
      is.na(flag) & verification_status == "PASS" ~ NA,
      is.na(flag) & verification_status == "FAIL" ~ "MANUAL FLAG",
      !is.na(flag) & verification_status == "PASS" ~ flag,
      !is.na(flag) & verification_status == "FAIL" ~ NA
    )
  ) %>%
  filter( !is.na(site),
    # Filter based on DT. The 2023 data is in MST.
    DT_round >= as.POSIXct("2023-01-01 00:00:00", tz = "MST") & DT_round <= as.POSIXct("2023-12-31 11:59:59", tz = "MST")
  ) %>%
  #fixing virridy site names
  mutate(site = gsub(" virridy","_virridy", site  ))%>%
  dplyr::select(DT_round, DT_join, site, parameter, mean = clean_mean, flag = clean_flag,last_site_visit) 
```

# Loading in 2024 dataset (verification in progress)

```{r}
#load in 2023 verified data
# Pulling in the manually verified data
verified_names <- list.files(
  here("data", "manual_data_verification", "2024_cycle", 
       "in_progress", "verified_directory"),
  full.names = T)

verified_dataset_2024 <- verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path)
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 
# Pulling in the pre verified data
pre_verified_names <- list.files(
  here("data", "manual_data_verification", "2024_cycle",
       "in_progress", "all_data_directory"),
  full.names = T)
pre_verified_dataset_2024 <- pre_verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path)
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 


# Combining all 2024 data by preferentially using post verified data
ver_names <- intersect(names(verified_dataset_2024), names(pre_verified_dataset_2024))
non_ver_names <- setdiff(names(pre_verified_dataset_2024), ver_names)
dataset_2024 <- c(
  verified_dataset_2024[ver_names], 
  pre_verified_dataset_2024[non_ver_names]
)
all_data_2024 <- dataset_2024 %>%
  bind_rows() %>% 
  data.table() %>%
  mutate(
    clean_mean = case_when(
      #verified data
      verification_status == "OMIT"  ~ NA,
      verification_status == "PASS" ~ mean,
      verification_status == "FLAGGED" ~ mean, 
      #unverified data
      is.na(verification_status) & is.na(flag) ~ mean, # no auto flag and not verified
      is.na(verification_status) & !is.na(flag) ~ NA #  auto flag and not verified
    ),
    clean_flag = case_when(
      #verified data
      verification_status == "OMIT"  ~ NA,
      verification_status == "PASS" ~  NA,
      verification_status == "FLAGGED" ~ user_flag,
      #unverified data
      is.na(verification_status) & is.na(flag) ~ NA, # no auto flag and not verified
      is.na(verification_status) & !is.na(flag) ~ flag #  auto flag and not verified
    )
  ) %>%
  filter( !is.na(site),
    # Filter based on DT. The 2024 data is in MST.
    DT_round >= as.POSIXct("2024-01-01 00:00:00", tz = "MST") & DT_round <= as.POSIXct("2024-12-31 11:59:59", tz = "MST")
  ) %>%
  mutate(last_site_visit = with_tz(last_site_visit, tzone = "MST"))%>%
  dplyr::select(DT_round, DT_join, site, parameter, mean = clean_mean, flag = clean_flag, last_site_visit) 
```

## 2025 dataset (auto qaqc only)

Skip to `Loading in pre-processed 2025 data` if the auto qaqc pipeline has already been run for 2025

### Non Hydro Vu site pull

These sites do not livestream to HydroVu so all the data is stored on Vulink or AquaTroll logs. The Vulink logs cannot be uploaded to HydroVu so we will pull them in manually here

```{r}

source("src/parse_insitu_html_log.R")
#sites that do not livestream
sites <- c("joei", "cbri","chd", "pfal", "sfm", "pbr", "pman") 

troll_files <- list.files(path = "data/sensor_data/2025/", pattern = "troll", full.names = T)%>%
  #only look for our upper sites that do not livestream in any capacity
  str_subset(paste(sites, collapse = "|"))

troll_data <- map(troll_files, parse_insitu_html_log) %>%
  bind_rows()

vulink_files <- list.files(path = "data/sensor_data/2025/", pattern = "vulink", full.names = T)%>%
  #only look for our upper sites that do not livestream in any capacity
  str_subset(paste(sites, collapse = "|"))

vulink_data <- map(vulink_files, parse_insitu_html_log) %>%
  bind_rows()

#always start with troll data and then fill in where needed with vulink data
log_data_2025 <- troll_data %>%
  #find the data that is not in the troll data but is in the vulink log and bind in
  bind_rows(anti_join(vulink_data, ., by = c("site", "parameter", "DT_join")))%>%
  #check for duplicates
   distinct(.keep_all = TRUE) %>%
# #format to match other data
  split(f = list(.$site, .$parameter), sep = "-") %>%
   keep(~nrow(.) > 0)


```

### HydroVu Pull

#### Setup

```{r}
# Configure your directory paths
staging_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pull/raw_data"  # Where raw data will be stored
flagged_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pullflagged_data" # Where flagged data will be saved
temp_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pull/temp_files" # Temporary processing files
final_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pull/final_output"       # Final processed data

# Configure your threshold files
sensor_thresholds_file <- "data/field_notes/qaqc/sensor_spec_thresholds.yml"
seasonal_thresholds_file <- "data/field_notes/qaqc/updated_seasonal_thresholds_2025_sjs.csv" #updated sensor thresholds with some manual changes for sites with minimal data

# Configure your credentials files
mwater_creds_file <- "creds/mWaterCreds.yml"
hydrovu_creds_file <- "creds/HydroVuCreds.yml"

# Configure date range for data retrieval
start_date <- "2025-01-01 00:00:00"  # MST
end_date <- "2025-07-05 23:59:59"    # MST

time_zone = "America/Denver"

# Configure sites to process
sites_to_process <- c("archery",  "boxcreek", "cottonwood", "elc",  
                       "pbd", "riverbluffs", "riverbend", "salyer", 
                      "springcreek", "udall")

# Configure parallel processing
max_workers <- 4  
# Maximum number of parallel workers# set up parallel processing
num_workers <- min(availableCores() - 1, max_workers)
plan(multisession, workers = num_workers)
furrr_options(
  globals = TRUE,
  packages = c("arrow", "data.table", "httr2", "tidyverse", "lubridate", "zoo",
               "padr", "stats", "RcppRoll", "yaml", "here", "fcw.qaqc")
)

# suppress scientific notation for consistent formatting
options(scipen = 999)
# read threshold data
sensor_thresholds <- read_yaml(sensor_thresholds_file)
season_thresholds <- read_csv(seasonal_thresholds_file, show_col_types = FALSE)

# read API credentials
mWater_creds <- read_yaml(mwater_creds_file)
hv_creds <- read_yaml(hydrovu_creds_file)

# authenticate access to HydroVu API
hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]))


# pull field data from mWater API
mWater_data <- load_mWater(creds = mWater_creds)

# grab field notes with proper timezone handling
all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data) %>%
  mutate(DT_round = with_tz(DT_round, tzone = "UTC"),
         last_site_visit = with_tz(last_site_visit, tzone = "UTC"),
         DT_join = as.character(DT_round))

# grab sensor malfunction records
sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data) %>%
  mutate(start_DT = with_tz(start_DT, tzone = "UTC"),
         end_DT = with_tz(end_DT, tzone = "UTC"))
```

#### Pulling 2025 HV data

```{r}
###### ALREADY RUN for 07/2025 #######
# get HydroVu site information
# hv_sites <- hv_locations_all(hv_token) %>%
#   filter(!grepl("vulink", name, ignore.case = TRUE))

###### ALREADY RUN for 07/2025 #######

# convert dates to proper timezone
mst_start <- ymd_hms(start_date, tz = time_zone)
mst_end <- ymd_hms(end_date, tz = time_zone)


###### ALREADY RUN for 07/2025 #######
# source("src/api_puller.R")
# 
# # pull data for each site
# walk(sites_to_process,
#      function(site) {
#        message("Requesting HV data for: ", site)
#        api_puller(
#          site = site,
#          start_dt = with_tz(mst_start, tzone = "UTC"),
#          end_dt = with_tz(mst_end, tzone = "UTC"),
#          api_token = hv_token,
#          dump_dir = staging_directory, 
#         hv_sites_arg = hv_sites, 
#         network = "all"
#        )
#      }
# )
##### Already RUN for 07/2025 ######

# load all raw data files
hv_data <- list.files(staging_directory, full.names = TRUE) %>%
  future_map_dfr(function(file_path){
    site_df <- read_parquet(file_path, as_data_frame = TRUE)
    return(site_df)
  }, .progress = TRUE)

# preprocess and standardize data
hv_data_2025 <- hv_data %>%
  data.table() %>%
  dplyr::select(-id) %>%
  mutate(units = as.character(units)) %>%
  filter(!grepl("vulink", name, ignore.case = TRUE),
         #only keep parameters of interest
         grepl(parameter_level_string, parameter, ignore.case = TRUE),
         #remove pH MV and Level
         !grepl("Level|MV", parameter, ignore.case = T)) %>%
  mutate(
    DT = timestamp,
    DT_round = round_date(DT, "15 minutes"),
    DT_join = as.character(DT_round),
    site = tolower(site)
  ) %>%
  dplyr::select(-name) %>%
  distinct(.keep_all = TRUE) %>%
  # split into site-parameter combinations for parallel processing
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)
```

### Auto QAQC pipeline for 2025 data

```{r}
# tidy raw data (default 15-minute intervals)

data_2025 <- c(hv_data_2025, log_data_2025)

tidy_data <- data_2025 %>%
  future_map(~tidy_api_data(api_data = .), .progress = TRUE) %>%
  keep(~!is.null(.))

# add field notes to tidied data
combined_data <- tidy_data %>%
  future_map(~add_field_notes(df = ., notes = all_field_notes), .progress = TRUE)

# generate summary statistics
summarized_data <- combined_data %>%
  map(~generate_summary_statistics(.))

# process data in chunks for memory efficiency
summarized_data_chunks <- split(1:length(summarized_data),
                                ceiling(seq_along(1:length(summarized_data))/10))

single_sensor_flags <- list()

for (chunk_idx in seq_along(summarized_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(summarized_data_chunks), " ===")
  
  indices <- summarized_data_chunks[[chunk_idx]]
  chunk_data <- summarized_data[indices]
  
  # apply single-parameter flags
  chunk_results <- chunk_data %>%
    map(
      function(data) {
        flagged_data <- data %>%
          data.table(.) %>%
          # flag field visits
          add_field_flag(df = .) %>%
          # flag missing/NA values
          add_na_flag(df = .) %>%
          # flag dissolved oxygen noise patterns
          find_do_noise(df = .) %>%
          # flag repeating/stuck values
          add_repeat_flag(df = .) %>%
          # flag depth shifts (sonde movement)
          add_depth_shift_flag(df = ., level_shift_table = all_field_notes, post2024 = TRUE) %>%
          # flag sensor drift (FDOM, Chl-a, Turbidity)
          add_drift_flag(df = .)
        
        # apply sensor specification flags if thresholds exist
        if (unique(data$parameter) %in% names(sensor_thresholds)) {
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_spec_flag(df = ., spec_table = sensor_thresholds)
        }
        
        # apply seasonal threshold flags if available
        if (unique(data$parameter) %in% unique(season_thresholds$parameter)) {
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_seasonal_flag(df = ., threshold_table = season_thresholds)
        }
        
        return(flagged_data)
      },
      .progress = TRUE
    )
  
  single_sensor_flags <- c(single_sensor_flags, chunk_results)
  
  if (chunk_idx < length(summarized_data_chunks)) {
    gc()  # garbage collection between chunks
    Sys.sleep(0.1)
  }
}

# combine single-parameter flags by site
intrasensor_flags <- single_sensor_flags %>%
  rbindlist(fill = TRUE) %>%
  split(by = "site")

# process inter-parameter flags in chunks
intrasensor_data_chunks <- split(1:length(intrasensor_flags),
                                 ceiling(seq_along(1:length(intrasensor_flags))/3))

intrasensor_flags_list <- list()
for (chunk_idx in seq_along(intrasensor_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(intrasensor_data_chunks), " ===")
  
  indices <- intrasensor_data_chunks[[chunk_idx]]
  chunk_data <- intrasensor_flags[indices]
  
  chunk_results <- chunk_data %>%
    map(
      function(data) {
        flagged_data <- data %>%
          data.table() %>%
          # flag when water temperature below freezing
          add_frozen_flag(.) %>%
          # check for overlapping flags and resolve
          intersensor_check(.) %>%
          # flag potential sensor burial
          add_burial_flag(.) %>%
          # flag when sonde is above water surface
          add_unsubmerged_flag(.)
        
        return(flagged_data)
      }
    ) %>%
    rbindlist(fill = TRUE) %>%
    mutate(flag = ifelse(flag == "", NA, flag)) %>%
    split(f = list(.$site, .$parameter), sep = "-") %>%
    purrr::discard(~ nrow(.) == 0) %>%
    # add known sensor malfunction periods
    map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))
  
  intrasensor_flags_list <- c(intrasensor_flags_list, chunk_results)
  
  if (chunk_idx < length(intrasensor_data_chunks)) {
    gc()
    Sys.sleep(0.1)
  }
}

# save intermediate results
iwalk(intrasensor_flags_list, 
      ~write_csv(.x, file.path(temp_directory, paste0(.y, ".csv"))))

#custom network check while fcw.qaqc package is being updated
source("src/network_check.R")

# apply network-level quality control
network_flags <- intrasensor_flags_list %>%
  # network check compares patterns across sites
  purrr::map(~network_check(df = .,network = "all", intrasensor_flags_arg = intrasensor_flags_list)) %>%
  rbindlist(fill = TRUE) %>%
  # clean up flag column formatting
  tidy_flag_column() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  # add suspect data flags for isolated anomalies
  purrr::map(~add_suspect_flag(.)) %>%
  rbindlist(fill = TRUE)


# final data cleaning and preparation
v_final_flags <- network_flags %>%
  # Remove isolated suspect flags (single point anomalies)
  dplyr::mutate(auto_flag = ifelse(
    is.na(auto_flag), NA,
    ifelse(auto_flag == "suspect data" & 
           is.na(lag(auto_flag, 1)) & 
           is.na(lead(auto_flag, 1)), NA, auto_flag)
  )) %>%
  # select final columns
  dplyr::select(c("DT_round", "DT_join", "site", "parameter", "mean", "units", 
                  "n_obs", "spread", "auto_flag", "mal_flag", "sonde_moved",
                  "sonde_employed", "season", "last_site_visit")) %>%
  # clean up empty flags
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA, 
                                   ifelse(auto_flag == "", NA, auto_flag))) %>%
  # split back into site-parameter combinations
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)

# save final processed data
iwalk(v_final_flags, 
      ~write_csv(.x, file.path(final_directory, paste0(.y, ".csv"))))


```

### Loading in pre-processed 2025 data

If auto qaqc has already been run for 2025, skip to this chunk

```{r}

#load in 2025 unverified but Auto QAQC'ed data
# Pulling in the manually verified data

flagged_2025_files <- list.files(
  here("data", "manual_data_verification", "2025_cycle", 
       "hydro_vu_pull", "final_output"),
  full.names = T)

flagged_2025_dataset <- flagged_2025_files %>% 
  map(\(file_path){
    site_parameter_df <- read_csv(file_path, show_col_types = F)
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 


all_data_2025 <- flagged_2025_dataset %>%
  bind_rows() %>% 
  data.table() %>%
  mutate(
    clean_mean = case_when(
      #verified data
      !is.na(mal_flag)  ~ NA,
      !is.na(auto_flag) & parameter == "FDOM Fluorescence" & auto_flag == "drift" ~ mean, # FDOM over flagged for drift
      !is.na(auto_flag) & parameter == "FDOM Fluorescence" & auto_flag == "outside of seasonal range" ~ mean, # FDOM over flagged for drift
            !is.na(auto_flag) & parameter == "Turbidity" & auto_flag == "outside of seasonal range" & mean <= 10 & mean == 0 ~ mean, # Turbidity overflagged for turb at low values, 
        !is.na(auto_flag) & parameter == "Chl-a Fluorescence" & auto_flag == "outside of seasonal range" & mean <= 2 & site == "sfm" ~ mean, # SFM Chl-a overflagged in spring values
      !is.na(auto_flag) & auto_flag == "outside of seasonal range" & site == "chd" & parameter != "pH"~ mean, 
      # CHD overflagged across most parameters except pH
      
      is.na(auto_flag) & is.na(mal_flag) ~ mean),
    clean_flag = case_when(
      !is.na(mal_flag)  ~ mal_flag,
      !is.na(auto_flag) ~ auto_flag,
      is.na(auto_flag) & is.na(mal_flag) ~ NA
    )
  ) %>%
  filter( !is.na(site),
    # Filter based on DT. The 2025 data is in UTC
    DT_round >= as.POSIXct("2025-01-01 00:00:00", tz = "UTC") & DT_round <= as.POSIXct("2025-12-31 11:59:59", tz = "UTC")
  ) %>%
  #converting to MST for later use
  mutate(
    DT_round = with_tz(DT_round, tz = "MST"),
    DT_join = as.character(DT_round), 
    last_site_visit = with_tz(last_site_visit, "MST")
  )%>%
  dplyr::select(DT_round, DT_join, site, parameter, mean = clean_mean, flag = clean_flag, last_site_visit) 

```



# Bind all sensor data & create smoothed mean for turb/chl-a

```{r}
source("src/low_pass_filter.R")

all_data_2023_smooth <- all_data_2023 %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0) %>%
  map(~arrange(., DT_round) %>% low_pass_filter(.)) %>%
  bind_rows()

all_data_2024_smooth <- all_data_2024%>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
   keep(~nrow(.) > 0)%>%
  map(~arrange(., DT_round) %>% low_pass_filter(.)) %>%
  bind_rows()

all_data_2025_smooth <- all_data_2025%>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
   keep(~nrow(.) > 0)%>%
  map(~arrange(., DT_round) %>% low_pass_filter(.)) %>%
  bind_rows()

all_sensor_data <- bind_rows(all_data_2023_smooth, all_data_2024_smooth, all_data_2025_smooth)%>%
  mutate(hours_since_last_visit = as.numeric(difftime(DT_round, last_site_visit, units = "hours")), 
         value = case_when(
           !is.na(smoothed_mean) ~ smoothed_mean, 
           !is.na(mean) ~ mean, 
           is.na(mean)& is.na(smoothed_mean) ~ NA
         ))%>%
  filter(!is.na(value))
  
write_parquet(all_sensor_data, "data/upper_clp_dss/sensor/prepped/all_sensor_data_long.parquet")
```

# Plot to double check
```{r}
 all_sensor_data%>%
  #filter(parameter == "Turbidity")%>%
  filter( site %in% c("chd"))%>%
  filter(year(DT_round) == 2025)%>%
  ggplot(aes(x = DT_round))+
  geom_point( aes(y = mean, color = flag))+
  #geom_line(aes(y = smoothed_mean), color = "blue")+
  #scale_y_log10()+
  facet_wrap(  ~parameter, scales = "free_y")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
# Create wide sensor dataset

```{r}
all_sensor_data_wide <- all_sensor_data%>%
  filter(parameter %in% c("Chl-a Fluorescence","Specific Conductivity","Temperature","Turbidity","FDOM Fluorescence", "Depth"))%>%
  group_by(site, DT_round, parameter, hours_since_last_visit)%>%
  summarise(value = mean(value, na.rm = TRUE), .groups = "drop")%>%
  ungroup()%>%
  pivot_wider(id_cols = c("site", "DT_round", "hours_since_last_visit"), names_from = parameter, values_from = value)%>%
  mutate(across(where(is.list), ~ map_dbl(.x, ~ ifelse(is.null(.), NA, .))))%>%
  #remove rows when any of the values in the columns other than site, DT_round, Depth (not yet model parameter) are NA
  mutate( data_avail = ifelse((!is.na(`Chl-a Fluorescence`) & 
    !is.na(`Specific Conductivity`) & 
    !is.na(`Temperature`) & 
    !is.na(`Turbidity`) & 
    !is.na(`FDOM Fluorescence`)), TRUE, FALSE))

#save applicable data

min_date = format(min(all_sensor_data_wide$DT_round, na.rm = TRUE), "%Y-%m-%d")
max_date = format(max(all_sensor_data_wide$DT_round, na.rm = TRUE), "%Y-%m-%d")

write_parquet(all_sensor_data_wide, paste0("data/upper_clp_dss/sensor/prepped/all_sensor_data_",min_date, "_", max_date,".parquet"))
```




## FC Sensor Data

TODO:
- Read in 2023-2024 historical data (chl-a not livestreaming). 


- Read in 2025

