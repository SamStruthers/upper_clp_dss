---
title: "01_data_prep"
author: "Sam Struthers- CSU ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
source("src/setup_libraries.R")

```


# Clean up sensor data

merge 2023 + 2024 data for sites OI


```{r}
#load verified data
ver_dir <- "data/sensor/verified_directory"

#verified_data <- set_names(map(list.files(verified_path, full.names = TRUE), read_rds), list.files(verified_path))

ver_files <- list.files(ver_dir, full.names = TRUE)
# Function to process each file
grab_verified_data <- function(file_path) {
  # Extract the site name using the gsub method
  site_name <- gsub(paste0(ver_dir, "/"), "", file_path) %>% gsub("-.*", "", .)
  
  # Read the data and transform it
  single_site_param <- readRDS(file_path) %>%
    #exclude data that did not pass verification
    #filter(!is.na(mean_verified))%>%
    mutate(site = site_name) %>%
    select(-verification_status)
  
  return(single_site_param)
}

# Apply process_file function to each file and combine the results
verified_sensor_data <- map_dfr(ver_files, grab_verified_data)%>%
  mutate(date = as.Date(DT_round, tz = "MST")) 

ver_data_clean <- verified_sensor_data%>%
  #filter(site %nin% c("tamasag", "timberline", "prospect", "legacy"))%>%
  select(DT_round, site, parameter, value = mean_verified, DT_join, last_site_visit, week, y_w)%>%
  mutate(hours_since_last_visit = as.numeric(difftime(DT_round, last_site_visit, units = "hours")))%>%
  mutate(date = as.Date(DT_round), 
         year = year(DT_round)) %>%
  mutate(site = case_when(site == "river bluffs" ~ "riverbluffs", 
                          T ~site))


#load unverified data
non_ver_data <- read_feather("data/sensor/psn_2024_3.feather")%>%
  #filter(site %nin% c("tamasag", "timberline", "prospect", "legacy"))%>%
  mutate(DT_join = as.character(DT_round))


non_ver_data_clean <- non_ver_data%>%
  mutate(hours_since_last_visit = as.numeric(difftime(DT_round, last_site_visit, units = "hours")))%>%
  #super basic cleaning to create test models, this should be replaced later on!
  filter(!(auto_cleaned_flag_binary == 1 & parameter == "Chl-a Fluorescence" ))%>%
  filter(!(auto_cleaned_flag_binary == 1 & parameter == "Turbidity" ))%>%
  filter(!grepl(pattern = "sv|site visit", x = flag, ignore.case = TRUE))%>%
  filter(!grepl(pattern = "outside of seasonal range;\nslope violation", x = flag, ignore.case = TRUE))%>%
  filter(!grepl(pattern = "sonde unsubmerged", x = flag, ignore.case = TRUE))%>%
  filter(!grepl(pattern = "sensor malfunction", x = flag, ignore.case = TRUE))%>%
    mutate(
      year = year(DT_round),
      week = week(DT_round),
      y_w = paste(year, "-", week),
      day = yday(DT_round),
      y_d = paste(year, "-", day),
      date = as.Date(DT_round))



# Remove rows from non_ver_data_clean based on matches in ver_data_clean
filtered_non_ver_data <- non_ver_data_clean %>%
  anti_join(ver_data_clean, by = c("site", "parameter", "date"))

#plot the filterednonver data
# filtered_non_ver_data%>%
#   filter(site == "pbd")%>%
#   ggplot(aes(x = DT_round, y = value, color = flag))+
#   geom_point()+
#   facet_wrap(~parameter, scales = "free_y")+
#   theme_minimal()+
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))


all_sensor_data <- bind_rows(filtered_non_ver_data, ver_data_clean%>%filter(!is.na(value)))

```

# Plot to double check
```{r}
# 
# #make a plot of nonver_wide with notes for temperature, facet by site
# all_sensor_data%>%
#   filter(parameter == "Temperature")%>%
#   filter( site %in% c("pbd","lincoln", "prospect"))%>%
#   ggplot(aes(x = DT_round, y = value, color = site))+
#   geom_point()+
#   facet_wrap( site ~ parameter, scales = "free_y")+
#   theme_minimal()+
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
# Create wide sensor dataset

```{r}
all_sensor_data_wide <- all_sensor_data%>%
  filter(parameter %in% c("Chl-a Fluorescence","Specific Conductivity","Temperature","Turbidity","FDOM Fluorescence", "Depth"))%>%
  group_by(site, DT_round, parameter, hours_since_last_visit)%>%
  summarise(value = mean(value, na.rm = TRUE), .groups = "drop")%>%
  ungroup()%>%
  pivot_wider(id_cols = c("site", "DT_round", "hours_since_last_visit"), names_from = parameter, values_from = value)%>%
  mutate(across(where(is.list), ~ map_dbl(.x, ~ ifelse(is.null(.), NA, .))))%>%
  #remove rows when any of the values in the columns other than site and DT_round are NA
  filter(
    !is.na(`Chl-a Fluorescence`) & 
    !is.na(`Specific Conductivity`) & 
    !is.na(`Temperature`) & 
    !is.na(`Turbidity`) & 
    !is.na(`FDOM Fluorescence`)& 
    !is.na(Depth))%>%
  mutate(data_avail = 1)

#save applicable data

saveRDS(all_sensor_data_wide, "data/sensor/prepped/all_sensor_data.rds")
```



# pull in grab sample data

## ROSS

```{r}

#discrete sample data and location metadata from most recent pub

source("src/pull_ROSS_zenodo_data.R")
zenodo_data <- pull_ROSS_zenodo_data(data_version = "v2025.07.01", DOI = "15883685", save_folder_dir = "data/upper_clp_dss/ross_clp_chem")

# Grab the water chemistry dataset

ross_water_chem <- zenodo_data[["most_recent_chem"]]%>%
  mutate(site_code = tolower(site_code), 
         #fixing site names based on sonde deployments with ROSS/Virridy sondes
         site_code = case_when(site_code == "archery" & DT_mst <= ymd("2024-11-30") ~ "archery virridy",
                          site_code == "timberline" & DT_mst <= ymd("2024-11-30") ~ "timberline virridy",
                          site_code == "prospect" & DT_mst <= ymd("2024-11-30") ~ "prospect virridy",
                          T ~ site_code))%>%
  filter(site_code %in% unique(all_sensor_data$site) & DT_mst >= as_date("2023-09-01"))%>%
  dplyr::select(site_code, 
         Date, 
         DT_sample = DT_mst,
         TOC, TN, DOC, ChlA, NO3, NH4, PO4, SC,Cl, lab_turb = Turbidity, TSS,DT_mst_char)%>%
  mutate(collector = "ROSS")
```

## FC
```{r}

FC_chem <- read_csv("data/upper_clp_dss/fc_clp_chem/UCLP_CWQMP_database_2022-24.csv", show_col_types = F)%>%
  dplyr::select(site_code = ShortDesc, 
         Date, 
         Cl, 
         TOC, NO3_N, 
         PO4 = oPhos,
         TDS, 
         TN = TN_calc,
         lab_turb = Turbidity)%>%
  mutate(Date = as.POSIXct(Date, format = "%m/%d/%y"), 
         NO3 = NO3_N/ 0.2259, 
         SC = TDS/ 0.65,
         Cl = as.numeric(Cl), 
         lab_turb = as.numeric(lab_turb),
         TN = as.numeric(TN),
         ChlA = NA_integer_,
         DOC = NA_integer_, 
         NH4 = NA_integer_, 
         TSS = NA_integer_
         )%>%
  filter(site_code %in% c("CHD", 'PBD', "SFM") & Date >= as_date("2023-09-01"))%>%
  mutate(site_code = tolower(site_code), 
         #NEED TO GET READ DT
         DT_sample = ymd_hm(paste0(Date, " 12:00"), tz = "America/Denver"), 
         DT_mst_char = as.character(DT_sample))%>%
  dplyr::select(site_code, 
         Date, 
         DT_sample,
         TOC, TN, DOC, ChlA, NO3, NH4, PO4, SC,Cl, lab_turb, TSS,DT_mst_char)%>%
  mutate(collector = "FC")




#write_csv(FC_chem, "data/FC_clp_chem/FC_testing_data.csv")
```

## Combine data
```{r}

water_chem <- bind_rows(ross_water_chem, FC_chem)

```

## plot available data vs water chem

```{r}
# all_sensor_data_wide%>%
#   filter(year(DT_round) == 2024)%>%
#   ggplot(aes(x = DT_round, y = data_avail))+
#   geom_point()+
#   #create a vertical line when a sample from water chem was collected
#   geom_vline(data = water_chem%>%rename(site = site_code)%>%filter(year(Date) == 2024), aes(xintercept = DT_sample), color = "red")+
#   facet_wrap(~site, scales = "free_y")

```



```{r}
# Function to find the most recent sensor data for each water chem sample
get_most_recent_sensor_data <- function(water_chem, all_sensor_data_wide) {
  
  # Create an empty list to store results
  result_list <- list()
  
  # Loop through each row in water_chem
  for (i in 1:nrow(water_chem)) {
    # Get the current water chemistry row
    current_row <- water_chem[i, ]
    
    # Filter the sensor data for the current site
    site_data <- all_sensor_data_wide %>%
      filter(site == current_row$site_code) %>%
      filter(date(DT_round) == date(current_row$DT_sample)) %>%
      filter(DT_round >= current_row$DT_sample) # Ensure sensor data is later than water chem data
    
    # Find the most recent sensor data
    most_recent_sensor <- site_data %>%
      arrange(DT_round)%>%
      slice_head(n = 1)
    
    if (nrow(most_recent_sensor) == 0) {
     # If no matching sensor data found look backwards first
    site_data <- all_sensor_data_wide %>%
      filter(site == current_row$site_code) %>%
      filter(date(DT_round) == date(current_row$DT_sample)) %>%
      filter(DT_round < current_row$DT_sample) # Ensure sensor data is later than water chem data
    
    # Find the most recent sensor data
    most_recent_sensor <- site_data %>%
      arrange(DT_round) %>%
      slice_tail(n = 1)
    }
    
    
    #If still no matching data is found, add NA values for sensor columns
    if (nrow(most_recent_sensor) == 0) {
      most_recent_sensor <- tibble(
        site = current_row$site_code,
        DT_round = NA,
        `Chl-a Fluorescence` = NA,
        `Specific Conductivity` = NA,
        Temperature = NA,
        Turbidity = NA,
        `FDOM Fluorescence` = NA, 
        Depth = NA
      )
    }
    
    # Combine the current water chemistry row with the most recent sensor data
    result_list[[i]] <- cbind(current_row, most_recent_sensor)
  }
  
  
  # Combine the list of results into a single dataframe
  final_result <- bind_rows(result_list)
  
  return(final_result)
}

# Apply the function to get the most recent sensor data for each water chemistry sample
final_data <- get_most_recent_sensor_data(water_chem, all_sensor_data_wide)%>%
  filter(!is.na(DT_round))%>%
  #select(site, DT_sample, DT_round)%>%
  #calc difference
  #mutate(diff = as.numeric(difftime(DT_round, DT_sample, units = "hours")))
  select(mw_id = site_code, 
         sensor_datetime = DT_round, 
         lab_datetime = DT_sample,
         FDOM = `FDOM Fluorescence`,
         hrs_since_last_cleaning = hours_since_last_visit,
         Temp = Temperature,
         Sensor_Cond = `Specific Conductivity`,
         Sensor_Turb = Turbidity,
         Depth, 
         Chl_a = `Chl-a Fluorescence`,
         TOC, DOC,  TN, NO3, SC, Turbidity  = lab_turb, Conductivity = SC, Cl, collector)


```

### plot match up data set

```{r}
  ggplotly(ggplot()+
  geom_point(data = water_chem %>% rename(mw_id = site_code), aes(x = DT_sample, y = TOC), color = "grey")+
  geom_point(data = final_data, aes(x = lab_datetime, y = TOC, color = mw_id, shape = collector)))
  

#+
  #geom_smooth(method = "loess")+
  theme_minimal()+
  facet_wrap(~mw_id, scales = "free_y")
```



# Bind with Virridy data

```{r}

virridy_data <- read_csv("data/parsed_data_virridy.csv")%>%
  select(-"...1" )%>%
  mutate(mw_id = as.character(mw_id))%>%
  mutate(collector = "Virridy")
  
set.seed(123)
ross_sites <- tibble(mw_id = unique(final_data$mw_id))%>%
  mutate(id = sample(1:1000, nrow(.), replace = TRUE))

ROSS_model_data <- final_data%>%
  left_join(ross_sites, by = c("mw_id"))%>%
  mutate(sensor_datetime = as.character(sensor_datetime), 
         lab_datetime = as.character(lab_datetime), 
         mw_id = as.character(id))%>%
  select(-id)
  


all_modeled_data <- ROSS_model_data%>%
  bind_rows(virridy_data)

write_csv(ROSS_model_data, "data/parsed_data_ROSS.csv")
write_csv(all_modeled_data, "data/parsed_data_ROSS_virridy.csv")


```



## Scratch

```{r}

# nonver_wide <- non_ver_data%>%
#   pivot_wider(id_cols = c("site", "DT_round"), names_from = parameter, values_from = value)%>%
#   mutate(DT_join = as.character(DT_round))

#loop through each site and add in the field notes data
# 
# add_field_notes <- function(site_oi){
#   
#   if(site_oi =="archery virridy"){
#     field_notes <- field_notes%>%
#       mutate(site = "archery virridy")
#   }
#   
#   site_data <- nonver_wide%>%
#     filter(site == site_oi)%>%
#     left_join(field_notes%>%filter(site == site_oi)%>%select(-site), by = c("DT_join"))%>%
#    fill(last_site_visit, .direction = "down")
#   return(site_data)
# }
# 
# #map over the unique site in non_ver data to add in the field notes
# non_ver_data_clean <- map_dfr(unique(non_ver_data$site), add_field_notes)%>%
#   mutate(hours_since_last_visit = as.numeric(difftime(DT_round, last_site_visit, units = "hours")))%>%
#   filter(!is.na(hours_since_last_visit)) %>%
#   mutate(across(where(is.list), ~ map_dbl(.x, ~ ifelse(is.null(.), NA, .))))%>%
#   pivot_longer(cols = c("Chl-a Fluorescence":"Turbidity"), names_to = "parameter", values_to = "value")%>%
#   filter(!is.na(value))%>%
#   left_join(non_ver_data %>%select(-value, -sensor_malfunction), by = c("site","parameter", "DT_round"))%>%
#   filter(is.na(flag))%>% 
#     mutate(
#       year = year(DT_round),
#       week = week(DT_round),
#       y_w = paste(year, "-", week),
#       day = yday(DT_round),
#       y_d = paste(year, "-", day)
#       )%>%
#   mutate(date = as.Date(DT_round))

#rm(non_ver_data, nonver_wide, verified_sensor_data)


```

